{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Training-Decision-Tree\" data-toc-modified-id=\"Training-Decision-Tree-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Training Decision Tree</a></span></li><li><span><a href=\"#Making-Random-Forest\" data-toc-modified-id=\"Making-Random-Forest-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Making Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bootstaping\" data-toc-modified-id=\"Bootstaping-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Bootstaping</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-bootstraping-:\" data-toc-modified-id=\"What-is-bootstraping-:-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>What is bootstraping :</a></span></li></ul></li><li><span><a href=\"#Creating-Classifier-With-Bagging\" data-toc-modified-id=\"Creating-Classifier-With-Bagging-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Creating Classifier With Bagging</a></span></li><li><span><a href=\"#Deleting-Every-Attribute\" data-toc-modified-id=\"Deleting-Every-Attribute-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Deleting Every Attribute</a></span></li><li><span><a href=\"#Create-Random-Forest-Classifier\" data-toc-modified-id=\"Create-Random-Forest-Classifier-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Create Random Forest Classifier</a></span></li></ul></li><li><span><a href=\"#Questions\" data-toc-modified-id=\"Questions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Questions</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-bootstrapping-and-what-is-it's-compact-on-variance-?\" data-toc-modified-id=\"What-is-bootstrapping-and-what-is-it's-compact-on-variance-?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>What is bootstrapping and what is it's compact on variance ?</a></span></li><li><span><a href=\"#What-is-overfiting?-Why-decision-tree-overfit?-Bagging-trying-to-solve-what-problem?\" data-toc-modified-id=\"What-is-overfiting?-Why-decision-tree-overfit?-Bagging-trying-to-solve-what-problem?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>What is overfiting? Why decision tree overfit? Bagging trying to solve what problem?</a></span></li><li><span><a href=\"#What-is-random-forest-and-bagging-relation?-Random-forest-trying-to-solve-what-problem?\" data-toc-modified-id=\"What-is-random-forest-and-bagging-relation?-Random-forest-trying-to-solve-what-problem?-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>What is random forest and bagging relation? Random forest trying to solve what problem?</a></span></li><li><span><a href=\"#Concolusion-from-accuracies\" data-toc-modified-id=\"Concolusion-from-accuracies-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Concolusion from accuracies</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 4 - Ensemble Classification\n",
    "**Farzad Habibi - 810195383**\n",
    "\n",
    "In this project we will make a Random Forest classifier using `Bagging and Aggregation` methodology on Decision Tree classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "train_set = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select 20% of datas as test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = train_set.drop(['target'], axis=1), train_set['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 242 entries, 132 to 102\n",
      "Data columns (total 13 columns):\n",
      "age         242 non-null int64\n",
      "sex         242 non-null int64\n",
      "cp          242 non-null int64\n",
      "trestbps    242 non-null int64\n",
      "chol        242 non-null int64\n",
      "fbs         242 non-null int64\n",
      "restecg     242 non-null int64\n",
      "thalach     242 non-null int64\n",
      "exang       242 non-null int64\n",
      "oldpeak     242 non-null float64\n",
      "slope       242 non-null int64\n",
      "ca          242 non-null int64\n",
      "thal        242 non-null int64\n",
      "dtypes: float64(1), int64(12)\n",
      "memory usage: 26.5 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "decision_tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decision_tree_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute accuracy for a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Decision Tree is \u001b[1;33m0.819672131147541\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Accuracy for Decision Tree is \\x1b[1;33m{accuracy_score(y_test, y_pred)}\\x1b[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is bootstraping : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply bootstraping by random selection between numbers. We select every samples as new train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `np.random.randint` not found.\n"
     ]
    }
   ],
   "source": [
    "np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "samples_indidces = [np.random.randint(0, len(X_train), 150) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Classifier With Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement `RandomForestClassifier` class. This class follow base estimator model of sklearn.\n",
    "* In `fit` method we compute and make all of our decision trees according to our hyper-parameters in constructor.\n",
    "* In `predict` method we predict values with each model, and then select maximum number repeated for every of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode \n",
    "class BaggingClassifier():\n",
    "    def __init__(self, n_estimators=5, max_samples=150):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "    def fit(self, X, y):\n",
    "        self.samples_indices = [np.random.randint(0, len(X), self.max_samples) for i in range(self.n_estimators)]\n",
    "        self.models = [DecisionTreeClassifier().fit(X.iloc[sample_indices], y.iloc[sample_indices])\n",
    "                       for sample_indices in self.samples_indices]\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        self.all_predictions = np.array([model.predict(X) for model in self.models])\n",
    "        val, count = mode(self.all_predictions, axis = 0) \n",
    "        return val.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BaggingClassifier at 0x130ac0bd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_classifier = BaggingClassifier()\n",
    "bagging_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bagging_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Our Bagging Classifier is \u001b[1;33m0.8524590163934426\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for Our Bagging Classifier is \\x1b[1;33m{accuracy_score(y_test, y_pred)}\\x1b[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Every Attribute\n",
    "In this part we delete every attribiute and see what accuracy they got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Bagging Classifier without \u001b[1;33mage\u001b[0m is \u001b[1;33m0.8032786885245902\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33msex\u001b[0m is \u001b[1;33m0.8524590163934426\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mcp\u001b[0m is \u001b[1;33m0.7377049180327869\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mtrestbps\u001b[0m is \u001b[1;33m0.8360655737704918\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mchol\u001b[0m is \u001b[1;33m0.8360655737704918\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mfbs\u001b[0m is \u001b[1;33m0.7868852459016393\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mrestecg\u001b[0m is \u001b[1;33m0.7540983606557377\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mthalach\u001b[0m is \u001b[1;33m0.7540983606557377\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mexang\u001b[0m is \u001b[1;33m0.8360655737704918\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33moldpeak\u001b[0m is \u001b[1;33m0.8032786885245902\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mslope\u001b[0m is \u001b[1;33m0.7868852459016393\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mca\u001b[0m is \u001b[1;33m0.8852459016393442\u001b[0m\n",
      "Accuracy for Bagging Classifier without \u001b[1;33mthal\u001b[0m is \u001b[1;33m0.8688524590163934\u001b[0m\n",
      "Best col to drop is \u001b[1;33mca\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "max_col = ''\n",
    "for col in X_train.columns:\n",
    "    \n",
    "    bagging_classifier = BaggingClassifier().fit(X_train.drop([col], axis=1), y_train)\n",
    "    y_pred = bagging_classifier.predict(X_test.drop([col], axis=1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f'Accuracy for Bagging Classifier without \\x1b[1;33m{col}\\x1b[0m is \\x1b[1;33m{accuracy}\\x1b[0m')\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_col = col\n",
    "\n",
    "print(f'Best col to drop is \\x1b[1;33m{max_col}\\x1b[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Forest Classifier \n",
    "\n",
    "In this part we select 5 cols and fit our bagging classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(BaggingClassifier):\n",
    "    def __init__(self, n_estimators=5, max_samples=150, n_cols=5):\n",
    "        self.n_cols = n_cols\n",
    "        super().__init__(n_estimators, max_samples)\n",
    "    def fit(self, X, y):\n",
    "        self.cols = X.columns[np.random.randint(0, len(X.columns), self.n_cols)]\n",
    "        return super().fit(X[self.cols], y)\n",
    "    def predict(self, X):\n",
    "        return super().predict(X[self.cols])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classifier = RandomForestClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Our Random Forest Classifier is \u001b[1;33m0.8524590163934426\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for Our Random Forest Classifier is \\x1b[1;33m{accuracy_score(y_test, y_pred)}\\x1b[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is bootstrapping and what is it's compact on variance ?\n",
    "\n",
    "Bootstrapping means selection of data to train on estimators with replacement. Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but this also means that predictors end up being less correlated so the ensembleâ€™s variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred. However, if you have spare time and CPU power you can use cross-validation to evaluate both bagging and pasting and select the one that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is overfiting? Why decision tree overfit? Bagging trying to solve what problem?\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.\n",
    "\n",
    "Decision tree overfit because in high depth we have very few data points corresponding to our rules. And our model will overfited on our data. \n",
    "\n",
    "Bagging solve the overfiting problem. Bagging seperate input in fewer input points and train a decision tree with low depth factor on it. Then it doesn't train on all data points and can not overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is random forest and bagging relation? Random forest trying to solve what problem?\n",
    "\n",
    "Random forset consists of bagging with some reduction in features. On the other hand it make data smaller in features dimension and do bagging on sub models. \n",
    "\n",
    "Random forest also like bagging tries to stop overfiting. Because it got more data features and make our tree simpler. On the other hand, random forest reduce variance of data points. It also reduce execuation and train time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concolusion from accuracies\n",
    "\n",
    "* As we see because of ensemble type of bagging, it's got better accuracy from decision tree. Cause decision tree can be overfit. \n",
    "* Random forest also can get better accuracy if we have chance to select better features. Because it makes variance of data points fewer and tries to not overfit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
